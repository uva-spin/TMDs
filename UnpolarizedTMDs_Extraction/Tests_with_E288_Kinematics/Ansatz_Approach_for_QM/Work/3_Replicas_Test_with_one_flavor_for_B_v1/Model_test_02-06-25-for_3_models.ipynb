{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "467b5629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHAPDF 6.5.4 loading /home/ishara/LHAPDF/LHAPDF-install/share/LHAPDF/NNPDF40_nlo_as_01180/NNPDF40_nlo_as_01180_0000.dat\n",
      "NNPDF40_nlo_as_01180 PDF set, member #0, version 1; LHAPDF ID = 331700\n",
      "Computed A values saved to A_for_E288kinematics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishara/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lhapdf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "NNPDF4_nlo = lhapdf.mkPDF('NNPDF40_nlo_as_01180')\n",
    "data = pd.read_csv(\"../E288.csv\")\n",
    "alpha = 1/137\n",
    "\n",
    "def pdf(pdfset, flavor, x, QQ):\n",
    "    return pdfset.xfxQ(flavor, x, QQ)\n",
    "\n",
    "def S(k):\n",
    "    return np.exp(-k**2)\n",
    "\n",
    "def fDNNQ(QM, b=0.5):\n",
    "    return np.exp(-b * QM)\n",
    "\n",
    "def compute_A(x1, x2, qT, QM):\n",
    "    f_u_x1 = pdf(NNPDF4_nlo, 2, x1, QM) \n",
    "    f_ubar_x2 = pdf(NNPDF4_nlo, -2, x2, QM)\n",
    "    f_u_x2 = pdf(NNPDF4_nlo, 2, x2, QM)\n",
    "    f_ubar_x1 = pdf(NNPDF4_nlo, -2, x1, QM)\n",
    "\n",
    "    Sk_contribution = (1/2)*(np.pi)*(np.exp(-qT*qT/2))\n",
    "\n",
    "    fDNN_contribution = fDNNQ(QM)\n",
    "\n",
    "    ux1ubarx2_term = x1*x2*f_u_x1*f_ubar_x2*Sk_contribution\n",
    "    ubarx1ux2_term = x2*x1*f_u_x2*f_ubar_x1*Sk_contribution\n",
    "    FUU = (ux1ubarx2_term + ubarx1ux2_term) * fDNN_contribution\n",
    "    cross_section =  FUU*qT*((4*np.pi*alpha)**2)/(9*QM*QM*QM)\n",
    "    return cross_section\n",
    "\n",
    "\n",
    "x1_values = data['xA'].values\n",
    "x2_values = data['xB'].values\n",
    "qT_values = data['PT'].values\n",
    "QM_values = data['QM'].values\n",
    "\n",
    "\n",
    "A_values = np.array([\n",
    "    compute_A(x1, x2, qT, QM)\n",
    "    for x1, x2, qT, QM in zip(x1_values, x2_values, qT_values, QM_values)\n",
    "])\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'x1': x1_values,\n",
    "    'x2': x2_values,\n",
    "    'qT': qT_values,\n",
    "    'QM': QM_values,\n",
    "    'A': A_values\n",
    "})\n",
    "\n",
    "results_df.to_csv(\"pseudodataE288_BQM.csv\", index=False)\n",
    "print(\"Computed A values saved to A_for_E288kinematics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "848f45d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 13:42:33.729092: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-06 13:42:33.736002: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738870953.744632  286053 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738870953.747261  286053 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-06 13:42:33.756503: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'Models_withLHAPDF' created successfully!\n",
      "Folder 'Results_withLHAPDF' created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738870954.503128  286053 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6498 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 - Epoch 1/5000, Loss: 1.607e-09\n",
      "Model 1 - Epoch 101/5000, Loss: 1.376e-09\n",
      "Model 1 - Epoch 201/5000, Loss: 1.119e-09\n",
      "Model 1 - Epoch 301/5000, Loss: 9.251e-10\n",
      "Model 1 - Epoch 401/5000, Loss: 7.703e-10\n",
      "Model 1 - Epoch 501/5000, Loss: 6.409e-10\n",
      "Model 1 - Epoch 601/5000, Loss: 5.304e-10\n",
      "Model 1 - Epoch 701/5000, Loss: 4.360e-10\n",
      "Model 1 - Epoch 801/5000, Loss: 3.591e-10\n",
      "Model 1 - Epoch 901/5000, Loss: 2.957e-10\n",
      "Model 1 - Epoch 1001/5000, Loss: 2.454e-10\n",
      "Model 1 - Epoch 1101/5000, Loss: 2.071e-10\n",
      "Model 1 - Epoch 1201/5000, Loss: 1.799e-10\n",
      "Model 1 - Epoch 1301/5000, Loss: 1.596e-10\n",
      "Model 1 - Epoch 1401/5000, Loss: 1.439e-10\n",
      "Model 1 - Epoch 1501/5000, Loss: 1.319e-10\n",
      "Model 1 - Epoch 1601/5000, Loss: 1.236e-10\n",
      "Model 1 - Epoch 1701/5000, Loss: 1.159e-10\n",
      "Model 1 - Epoch 1801/5000, Loss: 1.095e-10\n",
      "Model 1 - Epoch 1901/5000, Loss: 1.060e-10\n",
      "Model 1 - Epoch 2001/5000, Loss: 1.030e-10\n",
      "Model 1 - Epoch 2101/5000, Loss: 1.002e-10\n",
      "Model 1 - Epoch 2201/5000, Loss: 9.772e-11\n",
      "Model 1 - Epoch 2301/5000, Loss: 9.547e-11\n",
      "Model 1 - Epoch 2401/5000, Loss: 9.378e-11\n",
      "Model 1 - Epoch 2501/5000, Loss: 9.213e-11\n",
      "Model 1 - Epoch 2601/5000, Loss: 9.052e-11\n",
      "Model 1 - Epoch 2701/5000, Loss: 8.988e-11\n",
      "Model 1 - Epoch 2801/5000, Loss: 8.943e-11\n",
      "Model 1 - Epoch 2901/5000, Loss: 8.908e-11\n",
      "Model 1 - Epoch 3001/5000, Loss: 8.873e-11\n",
      "Model 1 - Epoch 3101/5000, Loss: 8.838e-11\n",
      "Model 1 - Epoch 3201/5000, Loss: 8.803e-11\n",
      "Model 1 - Epoch 3301/5000, Loss: 8.767e-11\n",
      "Model 1 - Epoch 3401/5000, Loss: 8.732e-11\n",
      "Model 1 - Epoch 3501/5000, Loss: 8.695e-11\n",
      "Model 1 - Epoch 3601/5000, Loss: 8.660e-11\n",
      "Model 1 - Epoch 3701/5000, Loss: 8.634e-11\n",
      "Model 1 - Epoch 3801/5000, Loss: 8.625e-11\n",
      "Model 1 - Epoch 3901/5000, Loss: 8.615e-11\n",
      "Model 1 - Epoch 4001/5000, Loss: 8.606e-11\n",
      "Model 1 - Epoch 4101/5000, Loss: 8.597e-11\n",
      "Model 1 - Epoch 4201/5000, Loss: 8.587e-11\n",
      "Model 1 - Epoch 4301/5000, Loss: 8.578e-11\n",
      "Model 1 - Epoch 4401/5000, Loss: 8.568e-11\n",
      "Model 1 - Epoch 4501/5000, Loss: 8.559e-11\n",
      "Model 1 - Epoch 4601/5000, Loss: 8.549e-11\n",
      "Model 1 - Epoch 4701/5000, Loss: 8.539e-11\n",
      "Model 1 - Epoch 4801/5000, Loss: 8.529e-11\n",
      "Model 1 - Epoch 4901/5000, Loss: 8.519e-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 - Epoch 5000/5000, Loss: 8.509e-11\n",
      "Model 1 saved successfully at Models_withLHAPDF/DNNQ_model_1.h5!\n",
      "Model 2 - Epoch 1/5000, Loss: 1.734e-09\n",
      "Model 2 - Epoch 101/5000, Loss: 1.481e-09\n",
      "Model 2 - Epoch 201/5000, Loss: 1.182e-09\n",
      "Model 2 - Epoch 301/5000, Loss: 9.682e-10\n",
      "Model 2 - Epoch 401/5000, Loss: 8.042e-10\n",
      "Model 2 - Epoch 501/5000, Loss: 6.724e-10\n",
      "Model 2 - Epoch 601/5000, Loss: 5.612e-10\n",
      "Model 2 - Epoch 701/5000, Loss: 4.670e-10\n",
      "Model 2 - Epoch 801/5000, Loss: 3.869e-10\n",
      "Model 2 - Epoch 901/5000, Loss: 3.200e-10\n",
      "Model 2 - Epoch 1001/5000, Loss: 2.659e-10\n",
      "Model 2 - Epoch 1101/5000, Loss: 2.232e-10\n",
      "Model 2 - Epoch 1201/5000, Loss: 1.914e-10\n",
      "Model 2 - Epoch 1301/5000, Loss: 1.664e-10\n",
      "Model 2 - Epoch 1401/5000, Loss: 1.491e-10\n",
      "Model 2 - Epoch 1501/5000, Loss: 1.344e-10\n",
      "Model 2 - Epoch 1601/5000, Loss: 1.255e-10\n",
      "Model 2 - Epoch 1701/5000, Loss: 1.172e-10\n",
      "Model 2 - Epoch 1801/5000, Loss: 1.102e-10\n",
      "Model 2 - Epoch 1901/5000, Loss: 1.061e-10\n",
      "Model 2 - Epoch 2001/5000, Loss: 1.030e-10\n",
      "Model 2 - Epoch 2101/5000, Loss: 9.998e-11\n",
      "Model 2 - Epoch 2201/5000, Loss: 9.728e-11\n",
      "Model 2 - Epoch 2301/5000, Loss: 9.504e-11\n",
      "Model 2 - Epoch 2401/5000, Loss: 9.323e-11\n",
      "Model 2 - Epoch 2501/5000, Loss: 9.148e-11\n",
      "Model 2 - Epoch 2601/5000, Loss: 9.013e-11\n",
      "Model 2 - Epoch 2701/5000, Loss: 8.965e-11\n",
      "Model 2 - Epoch 2801/5000, Loss: 8.924e-11\n",
      "Model 2 - Epoch 2901/5000, Loss: 8.887e-11\n",
      "Model 2 - Epoch 3001/5000, Loss: 8.850e-11\n",
      "Model 2 - Epoch 3101/5000, Loss: 8.812e-11\n",
      "Model 2 - Epoch 3201/5000, Loss: 8.774e-11\n",
      "Model 2 - Epoch 3301/5000, Loss: 8.735e-11\n",
      "Model 2 - Epoch 3401/5000, Loss: 8.696e-11\n",
      "Model 2 - Epoch 3501/5000, Loss: 8.659e-11\n",
      "Model 2 - Epoch 3601/5000, Loss: 8.640e-11\n",
      "Model 2 - Epoch 3701/5000, Loss: 8.630e-11\n",
      "Model 2 - Epoch 3801/5000, Loss: 8.620e-11\n",
      "Model 2 - Epoch 3901/5000, Loss: 8.610e-11\n",
      "Model 2 - Epoch 4001/5000, Loss: 8.600e-11\n",
      "Model 2 - Epoch 4101/5000, Loss: 8.590e-11\n",
      "Model 2 - Epoch 4201/5000, Loss: 8.580e-11\n",
      "Model 2 - Epoch 4301/5000, Loss: 8.570e-11\n",
      "Model 2 - Epoch 4401/5000, Loss: 8.559e-11\n",
      "Model 2 - Epoch 4501/5000, Loss: 8.549e-11\n",
      "Model 2 - Epoch 4601/5000, Loss: 8.538e-11\n",
      "Model 2 - Epoch 4701/5000, Loss: 8.527e-11\n",
      "Model 2 - Epoch 4801/5000, Loss: 8.517e-11\n",
      "Model 2 - Epoch 4901/5000, Loss: 8.506e-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 - Epoch 5000/5000, Loss: 8.495e-11\n",
      "Model 2 saved successfully at Models_withLHAPDF/DNNQ_model_2.h5!\n",
      "Model 3 - Epoch 1/5000, Loss: 1.821e-09\n",
      "Model 3 - Epoch 101/5000, Loss: 1.592e-09\n",
      "Model 3 - Epoch 201/5000, Loss: 1.306e-09\n",
      "Model 3 - Epoch 301/5000, Loss: 1.088e-09\n",
      "Model 3 - Epoch 401/5000, Loss: 9.090e-10\n",
      "Model 3 - Epoch 501/5000, Loss: 7.542e-10\n",
      "Model 3 - Epoch 601/5000, Loss: 6.272e-10\n",
      "Model 3 - Epoch 701/5000, Loss: 5.167e-10\n",
      "Model 3 - Epoch 801/5000, Loss: 4.231e-10\n",
      "Model 3 - Epoch 901/5000, Loss: 3.447e-10\n",
      "Model 3 - Epoch 1001/5000, Loss: 2.830e-10\n",
      "Model 3 - Epoch 1101/5000, Loss: 2.349e-10\n",
      "Model 3 - Epoch 1201/5000, Loss: 1.980e-10\n",
      "Model 3 - Epoch 1301/5000, Loss: 1.711e-10\n",
      "Model 3 - Epoch 1401/5000, Loss: 1.526e-10\n",
      "Model 3 - Epoch 1501/5000, Loss: 1.373e-10\n",
      "Model 3 - Epoch 1601/5000, Loss: 1.271e-10\n",
      "Model 3 - Epoch 1701/5000, Loss: 1.188e-10\n",
      "Model 3 - Epoch 1801/5000, Loss: 1.111e-10\n",
      "Model 3 - Epoch 1901/5000, Loss: 1.060e-10\n",
      "Model 3 - Epoch 2001/5000, Loss: 1.029e-10\n",
      "Model 3 - Epoch 2101/5000, Loss: 9.999e-11\n",
      "Model 3 - Epoch 2201/5000, Loss: 9.725e-11\n",
      "Model 3 - Epoch 2301/5000, Loss: 9.472e-11\n",
      "Model 3 - Epoch 2401/5000, Loss: 9.289e-11\n",
      "Model 3 - Epoch 2501/5000, Loss: 9.120e-11\n",
      "Model 3 - Epoch 2601/5000, Loss: 8.956e-11\n",
      "Model 3 - Epoch 2701/5000, Loss: 8.890e-11\n",
      "Model 3 - Epoch 2801/5000, Loss: 8.845e-11\n",
      "Model 3 - Epoch 2901/5000, Loss: 8.802e-11\n",
      "Model 3 - Epoch 3001/5000, Loss: 8.768e-11\n",
      "Model 3 - Epoch 3101/5000, Loss: 8.734e-11\n",
      "Model 3 - Epoch 3201/5000, Loss: 8.699e-11\n",
      "Model 3 - Epoch 3301/5000, Loss: 8.665e-11\n",
      "Model 3 - Epoch 3401/5000, Loss: 8.630e-11\n",
      "Model 3 - Epoch 3501/5000, Loss: 8.595e-11\n",
      "Model 3 - Epoch 3601/5000, Loss: 8.559e-11\n",
      "Model 3 - Epoch 3701/5000, Loss: 8.528e-11\n",
      "Model 3 - Epoch 3801/5000, Loss: 8.519e-11\n",
      "Model 3 - Epoch 3901/5000, Loss: 8.510e-11\n",
      "Model 3 - Epoch 4001/5000, Loss: 8.502e-11\n",
      "Model 3 - Epoch 4101/5000, Loss: 8.493e-11\n",
      "Model 3 - Epoch 4201/5000, Loss: 8.485e-11\n",
      "Model 3 - Epoch 4301/5000, Loss: 8.476e-11\n",
      "Model 3 - Epoch 4401/5000, Loss: 8.467e-11\n",
      "Model 3 - Epoch 4501/5000, Loss: 8.458e-11\n",
      "Model 3 - Epoch 4601/5000, Loss: 8.450e-11\n",
      "Model 3 - Epoch 4701/5000, Loss: 8.441e-11\n",
      "Model 3 - Epoch 4801/5000, Loss: 8.432e-11\n",
      "Model 3 - Epoch 4901/5000, Loss: 8.422e-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3 - Epoch 5000/5000, Loss: 8.413e-11\n",
      "Model 3 saved successfully at Models_withLHAPDF/DNNQ_model_3.h5!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_folders(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "        print(f\"Folder '{folder_name}' created successfully!\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_name}' already exists!\")\n",
    "\n",
    "models_folder = 'Models_withLHAPDF'\n",
    "results_folder = 'Results_withLHAPDF'\n",
    "create_folders(models_folder)\n",
    "create_folders(results_folder)\n",
    "alpha = 1/137\n",
    "\n",
    "data = pd.read_csv(\"pseudodataE288_BQM.csv\")\n",
    "pseudo_df = data\n",
    "x1_values = tf.constant(data['x1'].values, dtype=tf.float32)\n",
    "x2_values = tf.constant(data['x2'].values, dtype=tf.float32)\n",
    "qT_values = tf.constant(data['qT'].values, dtype=tf.float32)\n",
    "QM_values = tf.constant(data['QM'].values, dtype=tf.float32)\n",
    "A_true_values = tf.constant(data['A'].values, dtype=tf.float32)\n",
    "\n",
    "def DNNQ():\n",
    "    return models.Sequential([\n",
    "        layers.Input(shape=(1,)), \n",
    "        layers.Dense(100, activation='relu6'),\n",
    "        layers.Dense(300, activation='relu6'),\n",
    "        layers.Dense(300, activation='relu6'),\n",
    "        layers.Dense(250, activation='relu6'),\n",
    "        layers.Dense(250, activation='relu6'),\n",
    "        layers.Dense(1, activation='exponential')\n",
    "    ])\n",
    "\n",
    "def pdf(pdfset, flavor, x, QQ):\n",
    "    return pdfset.xfxQ(flavor, x, QQ)\n",
    "\n",
    "def custom_loss(dnnQ, A_true, x1, x2, qT, QM):\n",
    "    dnnQinputs = tf.reshape(QM, (-1, 1))\n",
    "    dnnQvals = dnnQ(dnnQinputs)\n",
    "\n",
    "    f_u_x1 = tf.constant(pdf(NNPDF4_nlo, 2, x1, QM), dtype=tf.float32)\n",
    "    f_ubar_x2 = tf.constant(pdf(NNPDF4_nlo, -2, x2, QM), dtype=tf.float32)\n",
    "    f_u_x2 = tf.constant(pdf(NNPDF4_nlo, 2, x2, QM), dtype=tf.float32)\n",
    "    f_ubar_x1 = tf.constant(pdf(NNPDF4_nlo, -2, x1, QM), dtype=tf.float32)\n",
    "\n",
    "    pi = tf.constant(np.pi, dtype=tf.float32)\n",
    "    Sk_contribution = (1 / 2) * pi * tf.exp(-qT**2 / 2)\n",
    "\n",
    "    ux1ubarx2_term = x1 * x2 * f_u_x1 * f_ubar_x2 * Sk_contribution\n",
    "    ubarx1ux2_term = x2 * x1 * f_u_x2 * f_ubar_x1 * Sk_contribution\n",
    "    FUU = ux1ubarx2_term + ubarx1ux2_term\n",
    "    cross_section = FUU * qT * ((4 * np.pi * alpha) ** 2) / (9 * QM * QM * QM) * dnnQvals\n",
    "    temploss = tf.abs(cross_section - A_true)\n",
    "    loss = tf.reduce_mean(temploss)  # MAE loss\n",
    "    return loss\n",
    "\n",
    "num_models = 3\n",
    "for i in range(1, num_models + 1):\n",
    "    dnnQ = DNNQ()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    epochs = 5000\n",
    "    print_epochs = 100\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = custom_loss(dnnQ, A_true_values, x1_values, x2_values, qT_values, QM_values)\n",
    "        grads = tape.gradient(loss, dnnQ.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, dnnQ.trainable_variables))\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "        if epoch % print_epochs == 0 or epoch == epochs - 1:\n",
    "            print(f\"Model {i} - Epoch {epoch + 1}/{epochs}, Loss: {loss.numpy():.3e}\")\n",
    "    \n",
    "    model_path = os.path.join(models_folder, f'DNNQ_model_{i}.h5')\n",
    "    dnnQ.save(model_path)\n",
    "    print(f\"Model {i} saved successfully at {model_path}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eda0ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
